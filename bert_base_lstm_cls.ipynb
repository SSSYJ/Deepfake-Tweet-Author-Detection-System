{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtmMKDhHyXCt"
      },
      "source": [
        "# COLX 585 Project 3.2\n",
        "## Generated text detection: Project 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMlrm1HZExLn"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHJfGAS1mz2l"
      },
      "source": [
        "Data: [TweepFake](https://www.kaggle.com/datasets/mtesconi/twitter-deep-fake-text)\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71iJHb9QNSb7",
        "outputId": "23f2934b-6879-4f41-e37c-3bbd56ccaff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9tEG9FaeOPN"
      },
      "source": [
        "## Import require Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FXGum0pBNmh9"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDkUmQOwExLr",
        "outputId": "f4f83a3d-4948-4f90-e511-c84b98ab715d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Tesla P100-PCIE-16GB\n"
          ]
        }
      ],
      "source": [
        "## Set seed of randomization and working device\n",
        "manual_seed = 77\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed(manual_seed)\n",
        "\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UG0ZuN6FksH",
        "outputId": "e866ef95-55da-4f3d-a8d7-688b7b7a8ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers\n",
        "! pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9T0DfuK7ExLy"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizerFast, AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0oyjhomExL7"
      },
      "source": [
        "## Data prepare\n",
        "\n",
        "Data is already split into train, validation and test:\n",
        "\n",
        " Split\t| # bot tweets |\t# human tweets |\ttotal \n",
        " ----- | ------------ | --------------- | ------ \n",
        " Training set |\t10354 |\t10358\t| 20712 \n",
        " Validation set |\t1152 |\t1150 |\t2302 \n",
        " Test set |\t1280 |\t1278 |\t2558 \n",
        "\n",
        " For bot tweets, GPT-2 (11 accounts, 3861 tweets), RNN (7 accounts, 4181 tweets), and Others (5 accounts, 4876 tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li616JkGiku2"
      },
      "source": [
        "First, we define a function to pre-process input data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b8L3V8MFExL8"
      },
      "outputs": [],
      "source": [
        "# define a function for data preparation\n",
        "def data_prepare(file_path, lab2ind, tokenizer, max_len = 32, mode = 'train'):\n",
        "    '''\n",
        "    file_path: the path to input file. \n",
        "                In train mode, the input must be a tsv file that includes two columns where the first is text, and second column is label.\n",
        "                The first row must be header of columns.\n",
        "\n",
        "                In predict mode, the input must be a tsv file that includes only one column where the first is text.\n",
        "                The first row must be header of column.\n",
        "\n",
        "    lab2ind: dictionary of label classes\n",
        "    tokenizer: BERT tokenizer\n",
        "    max_len: maximal length of input sequence\n",
        "    mode: train or predict\n",
        "    '''\n",
        "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
        "    if mode == 'train':\n",
        "        # Use pandas to load dataset\n",
        "        df = pd.read_csv(file_path, header=0, names=['account_name', 'content', 'account_type', 'label'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        labels = df.label.values\n",
        "        \n",
        "        # Create sentence and label lists\n",
        "        labels = [lab2ind[i] for i in labels] \n",
        "        print(\"Label is \", labels[0])\n",
        "        \n",
        "        # Convert data into torch tensors\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # if we are in predict mode, we will load one column (i.e., text).\n",
        "    elif mode == 'predict':\n",
        "        df = pd.read_csv(file_path, header=0, names=['account_name', 'content', 'account_type', 'label'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        # create placeholder\n",
        "        labels = []\n",
        "    else:\n",
        "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
        "        return\n",
        "        \n",
        "    # Create sentence and label lists\n",
        "    content = df.content.values\n",
        "\n",
        "    #### REF START ####\n",
        "\n",
        "    # We need to add a special token at the beginning for BERT to work properly.\n",
        "    content = [\"[CLS] \" + text for text in content]\n",
        "\n",
        "    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "    tokenized_texts = [tokenizer.tokenize(text) for text in content]\n",
        "    \n",
        "    # if the sequence is longer the maximal length, we truncate it to the pre-defined maximal length\n",
        "    tokenized_texts = [ text[:max_len+1] for text in tokenized_texts]\n",
        "\n",
        "    # We also need to add a special token at the end.\n",
        "    tokenized_texts = [ text+['[SEP]'] for text in tokenized_texts]\n",
        "    print (\"Tokenize the first sentence:\\n\",tokenized_texts[0])\n",
        "    \n",
        "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    print (\"Index numbers of the first sentence:\\n\",input_ids[0])\n",
        "\n",
        "    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "    print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for pad tokens\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Convert all of our data into torch tensors, the required datatype for our model\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "    #### REF END ####\n",
        "\n",
        "    return inputs, labels, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB2TKe6FkeJa"
      },
      "source": [
        "How the data should look like:\n",
        "```\n",
        "Tokenize the first sentence:\n",
        " ['[CLS]', 'it', 'was', 'my', 'birthday', ',', 'and', 'my', 'wife', 'and', 'daughter', 'surprised', 'me', 'with', 'some', 'surprise', 'guests', 'and', 'a', 'small', 'party', '.', '[SEP]']\n",
        "Index numbers of the first sentence:\n",
        " [101, 2009, 2001, 2026, 5798, 1010, 1998, 2026, 2564, 1998, 2684, 4527, 2033, 2007, 2070, 4474, 6368, 1998, 1037, 2235, 2283, 1012, 102]\n",
        "Index numbers of the first sentence after padding:\n",
        " [ 101 2009 2001 2026 5798 1010 1998 2026 2564 1998 2684 4527 2033 2007\n",
        " 2070 4474 6368 1998 1037 2235 2283 1012  102    0    0    0    0    0\n",
        "    0    0    0    0    0    0]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkjWo9hz0Sc6"
      },
      "source": [
        "We use [\"bert-base-uncased\"](https://github.com/google-research/bert) which refers to the **12-layer, 768-hidden, 12-heads, 110M parameters** [variant of BERT model](https://huggingface.co/bert-base-uncased). The vocabulary of \"bert-base-uncased\" was generated using bype-pair encoding and includes 30,522 WordPieces. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tr_NcxYJExL_"
      },
      "outputs": [],
      "source": [
        "model_path = \"bert-base-uncased\"\n",
        "# define label to number dictionary\n",
        "lab2ind = {'human': 0, 'gpt2': 1, 'rnn': 2, 'others': 3}\n",
        "\n",
        "# tokenizer from pre-trained BERT model\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path,do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Be1YacycXD"
      },
      "source": [
        "### Inspect the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHfYd6pduuR0",
        "outputId": "1abce245-b60a-4fa5-fc8c-0b9d1d25dde5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info about the train dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20712 entries, 0 to 20711\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   screen_name   20712 non-null  object\n",
            " 1   text          20712 non-null  object\n",
            " 2   account.type  20712 non-null  object\n",
            " 3   class_type    20712 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 647.4+ KB\n",
            "\n",
            "Label counts for the train dataset:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "human     10358\n",
              "others     3920\n",
              "rnn        3325\n",
              "gpt2       3109\n",
              "Name: class_type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data_folder_path = \"/content/drive/MyDrive/Colab Notebooks/data/TweepFake/\"\n",
        "train_df = pd.read_csv(data_folder_path+\"train.csv\")\n",
        "print('Info about the train dataset:')\n",
        "train_df.info()\n",
        "print('\\nLabel counts for the train dataset:')\n",
        "train_df['class_type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG7afEHryflP"
      },
      "source": [
        "### Extract train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN21IUc12UIJ",
        "outputId": "79997946-bf6a-4b34-a1f4-ca0de34adce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sample:\n",
            "Data size  (20712, 4)\n",
            "Label is  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2194 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'ye', '##a', 'now', 'that', 'note', 'good', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 6300, 2050, 2085, 2008, 3602, 2204, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [ 101 6300 2050 2085 2008 3602 2204  102    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "\n",
            "Validation sample:\n",
            "Data size  (2302, 4)\n",
            "Label is  0\n",
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'tight', ',', 'tight', ',', 'tight', ',', 'yeah', '!', '!', '!', 'https', ':', '/', '/', 't', '.', 'co', '/', 'w', '##j', '##3', '##n', '##v', '##ppa', '##sw', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 4389, 1010, 4389, 1010, 4389, 1010, 3398, 999, 999, 999, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1059, 3501, 2509, 2078, 2615, 13944, 26760, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [  101  4389  1010  4389  1010  4389  1010  3398   999   999   999 16770\n",
            "  1024  1013  1013  1056  1012  2522  1013  1059  3501  2509  2078  2615\n",
            " 13944 26760   102     0     0     0     0     0     0     0]\n",
            "\n",
            "Test sample:\n",
            "Data size  (2558, 4)\n",
            "Label is  0\n",
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'justin', 'timber', '##lake', 'really', 'one', 'of', 'the', 'goats', 'if', 'you', 'think', 'about', 'it', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 6796, 7227, 14530, 2428, 2028, 1997, 1996, 17977, 2065, 2017, 2228, 2055, 2009, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [  101  6796  7227 14530  2428  2028  1997  1996 17977  2065  2017  2228\n",
            "  2055  2009   102     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "# Use defined funtion to extract data\n",
        "print('Train sample:')\n",
        "train_inputs, train_labels, train_masks = data_prepare(data_folder_path+\"train.csv\", lab2ind,tokenizer)\n",
        "print('\\nValidation sample:')\n",
        "validation_inputs, validation_labels, validation_masks = data_prepare(data_folder_path+\"validation.csv\", lab2ind,tokenizer)\n",
        "print('\\nTest sample:')\n",
        "test_inputs, test_labels, test_masks = data_prepare(data_folder_path+\"test.csv\", lab2ind,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lcPwbokicgT",
        "outputId": "84fd8999-45a0-483d-be7a-c611b3210dac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20712, 34])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_inputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeQSqRAtymmn"
      },
      "source": [
        "### Create DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNM2geLmi0fG"
      },
      "source": [
        "Create an iterator of our data with `torch DataLoader`. This helps us to save on memory during training.\n",
        "\n",
        "For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32. We use 32 batch size here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WFyS9X1ZXt6p"
      },
      "outputs": [],
      "source": [
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CU6Xq9C2ExMC"
      },
      "outputs": [],
      "source": [
        "# We'll take training samples in random order in each epoch. \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, \n",
        "                              sampler = RandomSampler(train_data), # Select batches randomly\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "# We'll just read validation set sequentially.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_dataloader = DataLoader(validation_data, \n",
        "                                   sampler = SequentialSampler(validation_data), # Pull out batches sequentially.\n",
        "                                   batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                             sampler = SequentialSampler(test_data), # Pull out batches sequentially.\n",
        "                             batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ2s-6DeExMF"
      },
      "source": [
        "## Loading pre-trained model example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cps8ti7hQDML",
        "outputId": "f5ca38ef-f29f-419e-a5a2-e83dc18ffbe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfY7kczF3B0h"
      },
      "source": [
        "## Creating `Bert_cls` class\n",
        "\n",
        "Now, we put everthing together. We bulid a `Bert_cls` class to train a BERT classifier end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9nylGQAtON0N"
      },
      "outputs": [],
      "source": [
        "class Bert_cls(nn.Module):\n",
        "\n",
        "    def __init__(self, lab2ind, model_path, bert_hidden_size, lstm_hidden_size, vocab_size, embedding_size, num_layers, dropout):\n",
        "        super(Bert_cls, self).__init__()\n",
        "        self.model_path = model_path\n",
        "        self.bert_hidden_size = bert_hidden_size\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.hidden_size = self.bert_hidden_size + self.lstm_hidden_size\n",
        "        # self.bert_hidden_size = bert_hidden_size\n",
        "        # self.rnn_hidden_size = rnn_hidden_size\n",
        "        # self.rnn_layer_num = rnn_layer_num\n",
        "        # self.bidirectional = bidirectional\n",
        "\n",
        "        self.bert_model = BertModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
        "        self.lstm = nn.LSTM(bidirectional=True, input_size=embedding_size, hidden_size=lstm_hidden_size, num_layers=num_layers)\n",
        "\n",
        "\n",
        "        self.label_num = len(lab2ind)\n",
        "        \n",
        "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.label_num)\n",
        "\n",
        "    def forward(self, bert_ids, bert_mask):\n",
        "        bert_outputs = self.bert_model(input_ids=bert_ids, attention_mask = bert_mask)\n",
        "        pooler_output = bert_outputs['pooler_output']\n",
        "\n",
        "        embeds = self.embedding(bert_ids).permute(1, 0, 2)\n",
        "        _, (lstm_outputs, _) = self.lstm(embeds)\n",
        "        # lstm_output = torch.cat((lstm_outputs[0], lstm_outputs[1]), dim=1)\n",
        "        # lstm_outputs, _ = self.lstm(embeds)\n",
        "        lstm_output = torch.mean(lstm_outputs, dim=0)\n",
        "\n",
        "        output = torch.cat((pooler_output, lstm_output), dim=1)\n",
        "        \n",
        "        bert_attentions = bert_outputs['attentions']\n",
        "\n",
        "        x = self.dense(output)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        fc_output = self.fc(x)\n",
        "\n",
        "        return fc_output, bert_attentions \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qCLSNFJ2xJO"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "o08-ntvk2xli"
      },
      "outputs": [],
      "source": [
        "# HIDDEN_SIZE = 768\n",
        "# HIDDEN_SIZE = 1024\n",
        "\n",
        "BERT_HIDDEN_SIZE = 768\n",
        "LSTM_HIDDEN_SIZE = 800\n",
        "EMBEDDING_SIZE = 300\n",
        "NUM_LAYERS = 1\n",
        "VOCAB_SIZE=30522\n",
        "\n",
        "# RNN_LAYER_NUM = 1\n",
        "# BIDIRECTIONAL = True\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# rnn_layer_num, bidirectional, dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFXk-8JWoQsX"
      },
      "source": [
        "Instantiate model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIC3uhaEExMF",
        "outputId": "ad4fce12-8597-4fc3-9a26-bac5ddafba46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "bert_model = Bert_cls(lab2ind, \n",
        "                      model_path, \n",
        "                      # HIDDEN_SIZE,\n",
        "                      BERT_HIDDEN_SIZE, \n",
        "                      LSTM_HIDDEN_SIZE, \n",
        "                      VOCAB_SIZE,\n",
        "                      EMBEDDING_SIZE,\n",
        "                      NUM_LAYERS,\n",
        "                      # RNN_LAYER_NUM,\n",
        "                      # BIDIRECTIONAL,\n",
        "                      DROPOUT).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9lajIGeQ9m"
      },
      "source": [
        "---\n",
        "Not used\n",
        "\n",
        "`Transformers` library also provides a class, [`BertForSequenceClassification`](https://huggingface.co/transformers/master/model_doc/bert.html#bertforsequenceclassification), to automatically create classifier. Namly, this `bert_model` can be instantiated by \n",
        "\n",
        "```bert_model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=len(lab2ind)).to(device)```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV7ES5nqokUJ"
      },
      "source": [
        "Count the number of parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5zZMwKMokwe",
        "outputId": "4b6d24ad-56bd-4208-8731-649ce77b32ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 128,158,108 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(bert_model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE7Dw_3VExMJ"
      },
      "source": [
        "## Optimizer and Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyx2_qG2ExMK"
      },
      "source": [
        "For the purposes of fine-tuning, the authors recommend the following hyperparameter ranges (from Appendix A.3 of the [paper](https://arxiv.org/pdf/1810.04805.pdf)):\n",
        "\n",
        "* Batch size: 16, 32\n",
        "* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "* Number of epochs: 2, 3, 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6jvvyZBYqWX"
      },
      "source": [
        "Hypterparameters for BERT large model:\n",
        "\n",
        "* Batch size: 32\n",
        "* Learning rate (Adam): 2e-5\n",
        "* Number of epochs: 3\n",
        "\n",
        "Hypterparameters for BERT base model:\n",
        "\n",
        "* Batch size: 8\n",
        "* Learning rate (Adam): 3e-5\n",
        "* Number of epochs: 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQFXSd4nExMK",
        "outputId": "6ca3d000-06d2-48a3-e5d6-b5c99a179b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Parameters:\n",
        "lr = 3e-5\n",
        "max_grad_norm = 1.0\n",
        "epochs = 3\n",
        "warmup_proportion = 0.1\n",
        "num_training_steps  = len(train_dataloader) * epochs\n",
        "num_warmup_steps = num_training_steps * warmup_proportion\n",
        "\n",
        "### In Transformers, optimizer and schedules are instantiated like this:\n",
        "# Note: AdamW is a class from the huggingface library\n",
        "# the 'W' stands for 'Weight Decay\"\n",
        "optimizer = AdamW(bert_model.parameters(), lr=lr, correct_bias=False)\n",
        "# schedules\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\n",
        "# We use nn.CrossEntropyLoss() as our loss function. \n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65tgu1WeExMV"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvI47019plpC"
      },
      "source": [
        "We define a `train()` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nmWjTgRRefUq"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "\n",
        "        outputs,_ = model(input_ids, input_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        # delete used variables to free GPU memory\n",
        "        del batch, input_ids, input_mask, labels\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    # free GPU memory\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXoagDUlpuhq"
      },
      "source": [
        "We define a `evaluate()` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1WP6nFZkfgpS"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            # Unpack the inputs from our dataloader\n",
        "            input_ids, input_mask, labels = batch\n",
        "\n",
        "            outputs,_ = model(input_ids, input_mask)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # delete used variables to free GPU memory\n",
        "            del batch, input_ids, input_mask\n",
        "            epoch_loss += loss.cpu().item()\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return epoch_loss / len(iterator), accuracy, f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G86QFF03hBkn"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "k9A7k-3872xc"
      },
      "outputs": [],
      "source": [
        "# create checkpoint directory\n",
        "import os\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/ckpt/TweepFake/'\n",
        "if os.path.exists(save_path) == False:\n",
        "    os.makedirs(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eixPjzgSExMW",
        "outputId": "06351bec-a20a-40de-b7dc-70fe03e371b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  33%|███▎      | 1/3 [03:39<07:18, 219.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [1/3], Train Loss: 0.4949, Validation Loss: 0.3631, Validation Accuracy: 0.8688, Validation F1: 0.8474\n",
            "Test Accuracy: 0.8612, Test F1: 0.8332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [07:22<03:41, 221.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [2/3], Train Loss: 0.2577, Validation Loss: 0.4161, Validation Accuracy: 0.8801, Validation F1: 0.8720\n",
            "Test Accuracy: 0.8808, Test F1: 0.8693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [11:05<00:00, 221.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [3/3], Train Loss: 0.1066, Validation Loss: 0.6751, Validation Accuracy: 0.8736, Validation F1: 0.8710\n",
            "Test Accuracy: 0.8796, Test F1: 0.8739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    train_loss = train(bert_model, train_dataloader, optimizer, scheduler, criterion)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(bert_model, validation_dataloader, criterion)\n",
        "    test_loss, test_acc, test_f1 = evaluate(bert_model, test_dataloader, criterion)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': bert_model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "        }\n",
        "\n",
        "    torch.save(state, save_path+\"BERT_base_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, epochs, train_loss, val_loss, val_acc, val_f1))\n",
        "    print('Test Accuracy: {:.4f}, Test F1: {:.4f}'.format(test_acc, test_f1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "bert_base_lstm_cls.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}